{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34a5b3e7-4902-4422-91cb-9f42d5dcf693",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import keras\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.utils import *\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "import glob\n",
    "import os\n",
    "import librosa\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=FutureWarning)\n",
    "import tensorflow_io as tfio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae8fc745-0feb-483d-b9c0-e4e1cedf44ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 44)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y, sr = librosa.load('data/Ahmed_Gamal.99.mp3')\n",
    "data = librosa.util.normalize(y)\n",
    "data = librosa.util.fix_length(data= data, size=sr)\n",
    "data = librosa.util.normalize(data)\n",
    "spec = librosa.feature.melspectrogram(y=data, sr=sr, hop_length=512, fmax=8000)\n",
    "spec_db = librosa.power_to_db(spec, ref= np.max)\n",
    "# plt.imshow(spec_db)\n",
    "spec_db.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13db1db4-7bf7-4afa-a261-478459ac6f22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 44)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_mask = tfio.audio.freq_mask(spec_db, param=10)\n",
    "# plt.imshow(freq_mask)\n",
    "freq_mask.numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ebf8c9e-dc1e-4f62-8b4f-650c669f5758",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 44)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spec_db.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3631735-7b3f-435a-b523-0fa4d831d09e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 44)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_mask.numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "708f2219-b982-440a-89a2-af8b81f28aea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob.glob('data/*')[150].split('.')[0] == 'data\\\\Ahmed_Abdelhady'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67a5a005-c0e9-4166-b731-e454de1c4c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aug(audio):\n",
    "    y, sr = librosa.load(audio)\n",
    "    data = librosa.util.normalize(y)\n",
    "    data = librosa.util.fix_length(data= data, size=sr)\n",
    "    data = librosa.util.normalize(data)\n",
    "    spec = librosa.feature.melspectrogram(y=data, sr=sr, hop_length=512, fmax=8000)\n",
    "    spec_db = librosa.power_to_db(spec, ref= np.max)\n",
    "    freq_mask = tfio.audio.freq_mask(spec_db, param=10)\n",
    "    return freq_mask.numpy()\n",
    "def load_data(audio):\n",
    "    y, sr = librosa.load(audio)\n",
    "    data = librosa.util.normalize(y)\n",
    "    data = librosa.util.fix_length(data= data, size=sr)\n",
    "    data = librosa.util.normalize(data)\n",
    "    spec = librosa.feature.melspectrogram(y=data, sr=sr, hop_length=512, fmax=8000)\n",
    "    spec_db = librosa.power_to_db(spec, ref= np.max)\n",
    "    return spec_db\n",
    "def aug_1(audio):\n",
    "    y, sr = librosa.load(audio)\n",
    "    data = librosa.util.normalize(y)\n",
    "    data = librosa.util.fix_length(data= data, size=sr)\n",
    "    data = librosa.util.normalize(data)\n",
    "    spec = librosa.feature.melspectrogram(y=data, sr=sr, hop_length=512, fmax=8000)\n",
    "    spec_db = librosa.power_to_db(spec, ref= np.max)\n",
    "    time_mask = tfio.audio.time_mask(spec_db, param=10)\n",
    "    return time_mask.numpy()\n",
    "\n",
    "dataset = []\n",
    "dataset_label = []\n",
    "aug_dataset = []\n",
    "aug_dataset_label = []\n",
    "for audio in glob.glob('data/*'):\n",
    "    if audio.split('.')[0] == 'data\\\\Ahmed_Gamal':\n",
    "        data = load_data(audio)\n",
    "        data = np.mean(data.T, axis=0)\n",
    "        dataset.append(data)\n",
    "        dataset_label.append(0)\n",
    "        data_aug1 = aug_1(audio)\n",
    "        data_aug1 = np.mean(data_aug1.T, axis=0)\n",
    "        aug_dataset.append(data_aug1)\n",
    "        aug_dataset_label.append(0)\n",
    "        data_aug = aug(audio)\n",
    "        data_aug = np.mean(data_aug.T, axis=0)\n",
    "        aug_dataset.append(data_aug)\n",
    "        aug_dataset_label.append(0)\n",
    "        \n",
    "    elif audio.split('.')[0] == 'data\\\\Ahmed_Abdelhady':\n",
    "        data = load_data(audio)\n",
    "        data = np.mean(data.T, axis=0)\n",
    "        dataset.append(data)\n",
    "        dataset_label.append(1)\n",
    "        data_aug1 = aug_1(audio)\n",
    "        data_aug1 = np.mean(data_aug1.T, axis=0)\n",
    "        aug_dataset.append(data_aug1)\n",
    "        aug_dataset_label.append(1)\n",
    "        data_aug = aug(audio)\n",
    "        data_aug = np.mean(data_aug.T, axis=0)\n",
    "        aug_dataset.append(data_aug)\n",
    "        aug_dataset_label.append(1)\n",
    "        \n",
    "    else:\n",
    "        data = load_data(audio)\n",
    "        data = np.mean(data.T, axis=0)\n",
    "        dataset.append(data)\n",
    "        dataset_label.append(2)\n",
    "        data_aug1 = aug_1(audio)\n",
    "        data_aug1 = np.mean(data_aug1.T, axis=0)\n",
    "        aug_dataset.append(data_aug1)\n",
    "        aug_dataset_label.append(2)\n",
    "        data_aug = aug(audio)\n",
    "        data_aug = np.mean(data_aug.T, axis=0)\n",
    "        aug_dataset.append(data_aug)\n",
    "        aug_dataset_label.append(2)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# aud_data = []\n",
    "# aud_label = []\n",
    "# aud1_data = []\n",
    "# aud1_label = []\n",
    "# dataset = []\n",
    "# dataset_label = []\n",
    "# for audio in glob.glob('data/*'):\n",
    "#     if audio.split('.')[1] == 'wav':\n",
    "#         data1 = load_data(audio)\n",
    "#         # data1 = np.mean(data1.T, axis=0)\n",
    "#         dataset.append(data1)\n",
    "#         dataset_label.append(0)\n",
    "#         data = aug(audio)\n",
    "#         # data = np.mean(data.T, axis=0)\n",
    "#         aud_data.append(data)\n",
    "#         aud_label.append(0)\n",
    "#         data = aug_1(audio)\n",
    "#         # data = np.mean(data.T, axis=0)\n",
    "#         aud1_data.append(data)\n",
    "#         aud1_label.append(0)\n",
    "        \n",
    "#     else:\n",
    "#         data1 = load_data(audio)\n",
    "#         # data1 = np.mean(data1.T, axis=0)\n",
    "#         dataset.append(data1)\n",
    "#         dataset_label.append(1)\n",
    "#         data = aug(audio)\n",
    "#         # data = np.mean(data.T, axis=0)\n",
    "#         aud_data.append(data)  \n",
    "#         aud_label.append(1)\n",
    "#         data = aug_1(audio)\n",
    "#         # data = np.mean(data.T, axis=0)\n",
    "#         aud1_data.append(data)\n",
    "#         aud1_label.append(1)\n",
    "\n",
    "\n",
    "\n",
    "# def aug(audio):\n",
    "#     rate = np.random.uniform(0.8, 1.2)\n",
    "#     audio = librosa.effects.time_stretch(audio, rate= rate)\n",
    "#     n_steps = np.random.randint(-2, 2)\n",
    "#     audio = librosa.effects.pitch_shift(audio, sr= 44100, n_steps=n_steps)\n",
    "\n",
    "\n",
    "# for audio in glob.glob('data/*'):\n",
    "#     data, sr = librosa.load(audio)\n",
    "#     data = librosa.util.normalize(data)\n",
    "#     data = librosa.util.fix_length(data= data, size=sr)\n",
    "#     data = librosa.feature.mfcc(y = data, sr= sr, n_mfcc=40)\n",
    "#     data = np.mean(data.T, axis=0)\n",
    "#     # data = np.expand_dims(data, axis= -1)\n",
    "#     if audio.split('.')[1] == 'wav':\n",
    "#         dataset.append([data, 0])\n",
    "#         data = aug(data)\n",
    "#         aud_data.append([data, 0])\n",
    "        \n",
    "#     else:\n",
    "#         dataset.append([data, 1])\n",
    "#         data = aug(data)\n",
    "#         aud_data.append([data, 1])       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4db71023-0766-4a14-b309-ccd0e8be1785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def aug(audio):\n",
    "#     rate = np.random.uniform(0.8, 1.2)\n",
    "#     audio = librosa.effects.time_stretch(audio, rate= rate)\n",
    "#     n_steps = np.random.randint(-2, 2)\n",
    "#     audio = librosa.effects.pitch_shift(audio, sr= 44100, n_steps=n_steps)\n",
    "#     return audio\n",
    "# aud_data = []\n",
    "# dataset = []\n",
    "# for audio in glob.glob('data/*'):\n",
    "#     data, sr = librosa.load(audio) \n",
    "#     data = librosa.feature.mfcc(y = data, sr= sr, n_mfcc=40)\n",
    "#     data = np.mean(data.T, axis=0)\n",
    "#     if audio.split('.')[1] == 'wav':\n",
    "#         dataset.append([data, 0])\n",
    "#         data = aug(data)\n",
    "#         aud_data.append([data, 0])\n",
    "        \n",
    "#     else:\n",
    "#         dataset.append([data, 1])\n",
    "#         data = aug(data)\n",
    "#         aud_data.append([data, 1])       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e240ccbb-1622-45f3-b55c-bcdf42b65ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset + aug_dataset\n",
    "label = dataset_label + aug_dataset_label\n",
    "df = pd.DataFrame(list(zip(data, label)), columns=['features', 'label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4a775b2-f663-4ca5-a612-6ddae2dbece9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-47.697636, -51.558815, -53.706223, -54.21703...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-39.48367, -43.731686, -47.19318, -44.385365,...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-36.00963, -43.539062, -47.78155, -43.49435, ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-46.472843, -50.72612, -46.495888, -42.50551,...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-42.250233, -47.51032, -51.88612, -52.780117,...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>[-53.239845, -49.75006, -44.67341, -35.987133,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>[-54.87914, -51.309204, -43.72039, -27.755302,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>[-47.85237, -44.500412, -37.40594, -24.526588,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898</th>\n",
       "      <td>[-47.540283, -39.46462, -31.58059, -30.717382,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>[-39.35542, -32.795345, -26.301872, -25.069014...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>900 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              features  label\n",
       "0    [-47.697636, -51.558815, -53.706223, -54.21703...      2\n",
       "1    [-39.48367, -43.731686, -47.19318, -44.385365,...      2\n",
       "2    [-36.00963, -43.539062, -47.78155, -43.49435, ...      2\n",
       "3    [-46.472843, -50.72612, -46.495888, -42.50551,...      2\n",
       "4    [-42.250233, -47.51032, -51.88612, -52.780117,...      2\n",
       "..                                                 ...    ...\n",
       "895  [-53.239845, -49.75006, -44.67341, -35.987133,...      0\n",
       "896  [-54.87914, -51.309204, -43.72039, -27.755302,...      0\n",
       "897  [-47.85237, -44.500412, -37.40594, -24.526588,...      0\n",
       "898  [-47.540283, -39.46462, -31.58059, -30.717382,...      0\n",
       "899  [-39.35542, -32.795345, -26.301872, -25.069014...      0\n",
       "\n",
       "[900 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82e06210-179e-403f-aa6c-26af961fd408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09717b72-b3e7-4fc4-9465-dd9787ecc9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(df['features'].to_list())\n",
    "y = np.array(df['label'].to_list())\n",
    "x = np.expand_dims(x, axis=-1)\n",
    "y = to_categorical(y)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.8, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d148ce5b-9edb-4d8e-928d-ca650c437588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c930cfb5-d041-4692-9294-29d4844de615",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((128,), (900, 128, 1))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].shape, x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4dde7d8e-2a79-4b54-9a4d-85fa55339704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(720, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4ef2b72-bbe2-4ba7-b752-861023621222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(720, 128, 1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aba0e7b7-e75b-4d67-bf4d-d9431be15ae9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99dd88b5-3b28-4f45-9660-67a6aa3f404e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "90/90 [==============================] - 6s 13ms/step - loss: 1.1737 - accuracy: 0.6389 - val_loss: 0.5161 - val_accuracy: 0.7889\n",
      "Epoch 2/10\n",
      "90/90 [==============================] - 1s 9ms/step - loss: 0.2869 - accuracy: 0.8875 - val_loss: 0.2352 - val_accuracy: 0.9000\n",
      "Epoch 3/10\n",
      "90/90 [==============================] - 1s 10ms/step - loss: 0.1340 - accuracy: 0.9528 - val_loss: 0.1855 - val_accuracy: 0.9000\n",
      "Epoch 4/10\n",
      "90/90 [==============================] - 1s 9ms/step - loss: 0.0525 - accuracy: 0.9861 - val_loss: 0.0615 - val_accuracy: 0.9778\n",
      "Epoch 5/10\n",
      "90/90 [==============================] - 1s 10ms/step - loss: 0.0281 - accuracy: 0.9903 - val_loss: 0.0242 - val_accuracy: 0.9889\n",
      "Epoch 6/10\n",
      "90/90 [==============================] - 1s 11ms/step - loss: 0.0673 - accuracy: 0.9819 - val_loss: 0.0776 - val_accuracy: 0.9722\n",
      "Epoch 7/10\n",
      "90/90 [==============================] - 1s 11ms/step - loss: 0.0218 - accuracy: 0.9931 - val_loss: 0.0095 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "90/90 [==============================] - 1s 11ms/step - loss: 0.0057 - accuracy: 1.0000 - val_loss: 0.0082 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "90/90 [==============================] - 1s 11ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.0041 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "90/90 [==============================] - 1s 11ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.0103 - val_accuracy: 0.9944\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x226d6aa9940>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape = x_train[0].shape\n",
    "\n",
    "# Define input layer\n",
    "inputs = Input(shape=input_shape)\n",
    "\n",
    "# Add convolutional layers\n",
    "x = Conv1D(32, 3, activation='relu')(inputs)\n",
    "x = MaxPooling1D(2)(x)\n",
    "x = Conv1D(64, 3, activation='relu')(x)\n",
    "x = MaxPooling1D(2)(x)\n",
    "x = Conv1D(128, 3, activation='relu')(x)\n",
    "x = MaxPooling1D(2)(x)\n",
    "\n",
    "# Add fully connected layers\n",
    "x = Flatten()(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "outputs = Dense(3, activation='softmax')(x)\n",
    "\n",
    "# Define model\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "59dbad3b-aecf-4821-a512-63b3ce437e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 208ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_data(r\"F:\\ai\\Ahemd Abdelhady\\data\\0_09_13.wav\")\n",
    "data = np.mean(data.T, axis=0)\n",
    "data = np.expand_dims(data, axis=-1)\n",
    "data = np.expand_dims(data, axis=0)\n",
    "\n",
    "np.argmax(model.predict(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b33b9157-028d-4aeb-b353-91810aa32f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# model.add(Dense(100, input_shape=(128, ), activation= 'relu'))\n",
    "# model.add(Dense(100, activation='relu'))\n",
    "# model.add(Dense(100, activation='relu'))\n",
    "# model.add(Dense(3, activation='softmax'))\n",
    "# model.summary()\n",
    "# model.compile(loss= 'categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "# model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae269281-2f4e-4867-b618-00ee240d515e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 21ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x_ = load_data('Ahemd Abdelhady/4444.mp3')\n",
    "# x_ = np.expand_dims(x_, axis = [1, -1])\n",
    "y, sr = librosa.load(r\"C:\\Users\\user\\OneDrive - Delta Academy for Science\\Desktop\\WhatsApp Audio 2023-05-14 at 16.17.17.waptt.opus\", duration=226)\n",
    "data = librosa.util.normalize(y)\n",
    "data = librosa.util.fix_length(data= data, size=sr)\n",
    "spec = librosa.feature.melspectrogram(y=data, sr=sr, hop_length=512, fmax=8000)\n",
    "spec_db = librosa.power_to_db(spec, ref= np.max)\n",
    "x_ = np.mean(spec_db.T, axis=0)\n",
    "x_ = np.expand_dims(x_, axis =  1)\n",
    "x_ = np.expand_dims(x_, axis =  0)\n",
    "np.argmax(model.predict(x_))\n",
    "# x_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1c80a866-1342-4fe5-90c6-b567bb14526b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 66ms/step\n",
      "Predicted class: 2\n",
      "1/1 [==============================] - 0s 107ms/step\n",
      "Predicted class: 2\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "Predicted class: 0\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "Predicted class: 2\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "Predicted class: 0\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "Predicted class: 0\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "Predicted class: 0\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "Predicted class: 0\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "Predicted class: 2\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "Predicted class: 1\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "Predicted class: 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 40\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sd\u001b[38;5;241m.\u001b[39mInputStream(channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, blocksize\u001b[38;5;241m=\u001b[39mchunk_size, samplerate\u001b[38;5;241m=\u001b[39mrate, callback\u001b[38;5;241m=\u001b[39maudio_callback):\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m---> 40\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "\n",
    "rate = 16000\n",
    "duration = 5 \n",
    "chunk_size = int(rate * duration) \n",
    "\n",
    "audio_buffer = np.zeros(chunk_size, dtype=np.float32)\n",
    "\n",
    "frame_count = 0\n",
    "\n",
    "def make_prediction():\n",
    "    global audio_buffer, frame_count\n",
    "\n",
    "    data = librosa.util.normalize(audio_buffer)\n",
    "    data = librosa.util.fix_length(data= data, size=rate)\n",
    "    data = librosa.util.normalize(data)\n",
    "    spec = librosa.feature.melspectrogram(y=data, sr=rate, hop_length=hop_length, n_fft=n_fft, n_mels= n_mels)\n",
    "    spec_db = librosa.power_to_db(spec, ref= np.max)\n",
    "    data = np.mean(spec_db.T, axis=0)\n",
    "    data = np.expand_dims(data, axis=-1)\n",
    "    data = np.expand_dims(data, axis=0)\n",
    "        \n",
    "    prediction = np.argmax(model.predict(data))\n",
    "\n",
    "    print(f\"Predicted class: {prediction}\")\n",
    "    audio_buffer = np.zeros(chunk_size, dtype=np.float32)\n",
    "    frame_count = 0\n",
    "\n",
    "def audio_callback(indata, frames, time, status):\n",
    "    global audio_buffer, frame_count\n",
    "    audio_buffer[frame_count * frames:(frame_count + 1) * frames] = indata[:, 0]\n",
    "    frame_count += 1\n",
    "    if frame_count * frames >= chunk_size:\n",
    "        make_prediction()\n",
    "with sd.InputStream(channels=1, blocksize=chunk_size, samplerate=rate, callback=audio_callback):\n",
    "    while True:\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# import pyaudio\n",
    "# import numpy as np\n",
    "# import librosa\n",
    "# import tensorflow as tf\n",
    "\n",
    "# p = pyaudio.PyAudio()\n",
    "\n",
    "# # Define the audio parameters\n",
    "# rate = 16000  # Sampling rate\n",
    "# chunk_size = 1024  # Number of samples per frame\n",
    "# n_fft = 2048  # FFT size for spectrogram computation\n",
    "# hop_length = 512  # Hop length for spectrogram computation\n",
    "# n_mels = 128  # Number of Mel frequency bins for spectrogram computation\n",
    "# duration = 60  # Duration of audio to capture (in seconds)\n",
    "\n",
    "# # Compute the number of chunks needed to capture the desired duration\n",
    "# num_chunks = int(rate / chunk_size * duration)\n",
    "\n",
    "# # Initialize an empty buffer to accumulate audio data\n",
    "# audio_buffer = np.zeros(num_chunks * chunk_size, dtype=np.float32)\n",
    "\n",
    "# # Open the microphone stream\n",
    "# stream = p.open(format=pyaudio.paFloat32, channels=1, rate=rate, input=True, frames_per_buffer=chunk_size)\n",
    "\n",
    "# # Loop to capture audio and make predictions\n",
    "# while True:\n",
    "#     # Read a frame of audio input from the stream\n",
    "#     data = stream.read(chunk_size)\n",
    "\n",
    "#     # Convert the audio data to a NumPy array\n",
    "#     audio = np.frombuffer(data, dtype=np.float32)\n",
    "\n",
    "#     # Add the audio data to the buffer\n",
    "#     audio_buffer = np.concatenate((audio_buffer[chunk_size:], audio))\n",
    "\n",
    "#     # If the buffer is full, preprocess the audio data and make a prediction\n",
    "#     if len(audio_buffer) >= num_chunks * chunk_size:\n",
    "#         # Compute the spectrogram of the audio data\n",
    "#         data = librosa.util.normalize(audio_buffer)\n",
    "#         data = librosa.util.fix_length(data= data, size=rate)\n",
    "#         data = librosa.util.normalize(data)\n",
    "#         spec = librosa.feature.melspectrogram(y=data, sr=rate, hop_length=hop_length, n_fft=n_fft, n_mels= n_mels)\n",
    "#         spec_db = librosa.power_to_db(spec, ref= np.max)\n",
    "#         data = np.mean(spec_db.T, axis=0)\n",
    "#         data = np.expand_dims(data, axis=-1)\n",
    "#         data = np.expand_dims(data, axis=0)\n",
    "        \n",
    "#         prediction = np.argmax(model.predict(data))\n",
    "\n",
    "#         # Print the predicted class label\n",
    "#         print(f\"Predicted class: {prediction}\")\n",
    "\n",
    "#         # Reset the audio buffer\n",
    "#         audio_buffer = np.zeros(num_chunks * chunk_size, dtype=np.float32)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# import pyaudio\n",
    "# import threading\n",
    "# p = pyaudio.PyAudio()\n",
    "# rate = 16000\n",
    "# chunksize = 1024\n",
    "# n_fft = 2048\n",
    "# hop_length = 512\n",
    "# n_mels = 128\n",
    "# duration = 5\n",
    "# audio_buffer = np.zeros(rate * duration, dtype= np.float32)\n",
    "# def load_data_realtime():\n",
    "#     global audio_buffer\n",
    "#     data = librosa.util.normalize(audio_buffer)\n",
    "#     data = librosa.util.fix_length(data= data, size=rate)\n",
    "#     data = librosa.util.normalize(data)\n",
    "#     spec = librosa.feature.melspectrogram(y=data, sr=rate, hop_length=hop_length, n_fft=n_fft, n_mels= n_mels)\n",
    "#     spec_db = librosa.power_to_db(spec, ref= np.max)\n",
    "#     data = np.mean(spec_db.T, axis=0)\n",
    "#     data = np.expand_dims(data, axis=-1)\n",
    "#     data = np.expand_dims(data, axis=0)\n",
    "#     print(np.argmax(model.predict(data)))\n",
    "#     audio_buffer = np.zeros(rate * duration, dtype= np.float32)\n",
    "# def timer_callback():\n",
    "#     threading.Timer(duration, timer_callback).start()\n",
    "#     load_data_realtime()\n",
    "\n",
    "# timer_callback()\n",
    "# stream = p.open(format=pyaudio.paFloat32, channels=1, rate=rate, input=True, frames_per_buffer=chunk_size)\n",
    "\n",
    "# while True:\n",
    "#     # Read a frame of audio input from the stream\n",
    "#     data = stream.read(chunk_size)\n",
    "\n",
    "#     # Convert the audio data to a NumPy array\n",
    "#     audio = np.frombuffer(data, dtype=np.float32)\n",
    "\n",
    "#     # Add the audio data to the buffer\n",
    "#     audio_buffer = np.concatenate((audio_buffer[chunk_size:], audio))\n",
    "\n",
    "#     # If the buffer is full, trigger the timer to make a prediction\n",
    "#     if len(audio_buffer) >= rate * duration:\n",
    "#         timer_callback()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# rate = 16000\n",
    "# chunk_size = 1024\n",
    "# duration = 2\n",
    "# num_chunks = int(rate / chunk_size * duration)\n",
    "# audio_buffer = np.zeros(num_chunks * chunk_size, dtype=np.int16)\n",
    "# stream = p.open(format=pyaudio.paInt16, channels=1, rate=rate, input=True, frames_per_buffer=chunk_size)\n",
    "# while True:\n",
    "#     data = stream.read(chunk_size)\n",
    "#     audio = np.frombuffer(data, dtype= np.int16)\n",
    "#     audio = audio.astype(np.float32) / 32767.0\n",
    "#     audio_buffer = np.concatenate((audio_buffer[chunk_size:], audio))\n",
    "\n",
    "    \n",
    "#     n_fft = 2048\n",
    "#     hop_length = 512\n",
    "#     n_mels = 128\n",
    "#     if len(audio_buffer) >= num_chunks * chunk_size:\n",
    "#         data = librosa.util.normalize(audio)\n",
    "#         data = librosa.util.fix_length(data= data, size=rate)\n",
    "#         spec = librosa.feature.melspectrogram(y=data, sr=rate, hop_length=512, n_fft= n_fft, n_mels= n_mels)\n",
    "#         spec_db = librosa.power_to_db(spec, ref= np.max)\n",
    "#         x_ = np.mean(spec_db.T, axis=0)\n",
    "#         x_ = np.expand_dims(x_, axis =  1)\n",
    "#         x_ = np.expand_dims(x_, axis =  0)\n",
    "#         print(np.argmax(model.predict(x_)))\n",
    "#         audio_buffer = np.zeros(num_chunks * chunk_size, dtype=np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "7433d033-1c8c-4085-a481-8b0db98f82ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer \"conv1d\" is incompatible with the layer: expected min_ndim=3, found ndim=2. Full shape received: (None, 128)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[199], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      2\u001b[0m model\u001b[38;5;241m=\u001b[39mSequential()\n\u001b[1;32m----> 3\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mConv1D\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msame\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_train\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39madd(MaxPooling1D(pool_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, strides \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m, padding \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msame\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m      6\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Conv1D(\u001b[38;5;241m256\u001b[39m, kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, strides\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msame\u001b[39m\u001b[38;5;124m'\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Ai_gpu\\lib\\site-packages\\tensorflow\\python\\trackable\\base.py:205\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m   result \u001b[38;5;241m=\u001b[39m method(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m previous_value  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Ai_gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Ai_gpu\\lib\\site-packages\\keras\\engine\\input_spec.py:250\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    248\u001b[0m     ndim \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;241m.\u001b[39mrank\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ndim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m ndim \u001b[38;5;241m<\u001b[39m spec\u001b[38;5;241m.\u001b[39mmin_ndim:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    251\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    252\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis incompatible with the layer: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    253\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected min_ndim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec\u001b[38;5;241m.\u001b[39mmin_ndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    254\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound ndim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    255\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFull shape received: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(shape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    256\u001b[0m         )\n\u001b[0;32m    257\u001b[0m \u001b[38;5;66;03m# Check dtype.\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: Input 0 of layer \"conv1d\" is incompatible with the layer: expected min_ndim=3, found ndim=2. Full shape received: (None, 128)"
     ]
    }
   ],
   "source": [
    "from keras import *\n",
    "model=Sequential()\n",
    "model.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu', input_shape=x_train[0].shape))\n",
    "model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n",
    "\n",
    "model.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n",
    "\n",
    "model.add(Conv1D(128, kernel_size=5, strides=1, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv1D(64, kernel_size=5, strides=1, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=32, activation='relu'))\n",
    "# model.add(Dropout(0.3))\n",
    "model.add(Dense(units=512, activation='relu'))\n",
    "model.add(Dense(units=1024, activation='relu'))\n",
    "\n",
    "\n",
    "model.add(Dense(units=2, activation='softmax'))\n",
    "model.compile(optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = ['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d417502a-67ae-45f2-af7d-ce8d51781d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "4/4 [==============================] - 1s 154ms/step - loss: 1.3339 - accuracy: 0.4643 - val_loss: 0.7559 - val_accuracy: 0.5000\n",
      "Epoch 2/20\n",
      "4/4 [==============================] - 0s 52ms/step - loss: 0.7960 - accuracy: 0.3750 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 3/20\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.7378 - accuracy: 0.4375 - val_loss: 0.6999 - val_accuracy: 0.5000\n",
      "Epoch 4/20\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.7069 - accuracy: 0.4821 - val_loss: 0.6954 - val_accuracy: 0.5000\n",
      "Epoch 5/20\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.7074 - accuracy: 0.4821 - val_loss: 0.6943 - val_accuracy: 0.5000\n",
      "Epoch 6/20\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.7009 - accuracy: 0.5000 - val_loss: 0.6943 - val_accuracy: 0.5000\n",
      "Epoch 7/20\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.6997 - accuracy: 0.5000 - val_loss: 0.6952 - val_accuracy: 0.5000\n",
      "Epoch 8/20\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.7009 - accuracy: 0.4464 - val_loss: 0.6934 - val_accuracy: 0.4643\n",
      "Epoch 9/20\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.7078 - accuracy: 0.4821 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 10/20\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.6942 - accuracy: 0.5000 - val_loss: 0.6976 - val_accuracy: 0.5000\n",
      "Epoch 11/20\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.7018 - accuracy: 0.5000 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 12/20\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.6861 - accuracy: 0.5893 - val_loss: 0.7330 - val_accuracy: 0.5000\n",
      "Epoch 13/20\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.7127 - accuracy: 0.5000 - val_loss: 0.6975 - val_accuracy: 0.5000\n",
      "Epoch 14/20\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.7126 - accuracy: 0.5000 - val_loss: 0.6963 - val_accuracy: 0.5000\n",
      "Epoch 15/20\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.6926 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 16/20\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 17/20\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.6934 - accuracy: 0.4286 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 18/20\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 19/20\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 20/20\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2ac30e6c550>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=20, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "18a1e323-a6cb-4612-a772-34225dc9189c",
   "metadata": {},
   "outputs": [],
   "source": [
    "voice, sr = librosa.load(r\"F:\\ai\\Ahemd Abdelhady\\data\\26.mp3\")\n",
    "voice = librosa.util.normalize(voice)\n",
    "voice = librosa.util.fix_length(data= voice, size=12000)\n",
    "voice = librosa.feature.mfcc(y = voice, sr= sr, n_mfcc=40)\n",
    "voice = np.mean(voice.T, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "35f26b62-ee7d-412f-8c28-5aaa8ec402ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 40)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voice = np.expand_dims(voice, axis=0)\n",
    "voice.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "1a79afde-ab70-4fdf-9999-1b361afcafd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 182ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(model.predict(voice))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d73c22-27a7-48b8-b983-39d2915b4707",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cfd033-5dd3-4c86-9125-6a101cce492c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9e6f2a41-6b2a-41fc-9bed-428865d34386",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = [[feature, label] for feature, label in zip(data, np.zeros(70, dtype= np.int16).tolist())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a46d09ce-0c9b-4875-b2cf-cb736112bd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(d, columns=['features', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e377c963-25d4-4689-8b6e-0acf91595dab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    70\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4d1796d7-0f97-4d10-bc87-690c26f53624",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_7088\\1628944672.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  x = np.array(df['features'].to_list())\n"
     ]
    }
   ],
   "source": [
    "x = np.array(df['features'].to_list())\n",
    "y = np.array(df['label'].to_list())\n",
    "x = np.expand_dims(x, axis=-1)\n",
    "y = to_categorical(y)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.8, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9bd0753c-1463-4015-8f11-1910814d71df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42,)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f54c48c1-edfa-4b87-a7c3-c5345ea3645d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_6 (Dense)             (None, 100)               4300      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 24,601\n",
      "Trainable params: 24,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(100, input_shape= (42,), activation= 'relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(1, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657ee4c6-18a0-4676-80c4-3e1b8f33b0a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "18d9d369-1feb-405e-aca2-826a5be25fa7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to find data adapter that can handle input: (<class 'list'> containing values of types {\"<class 'numpy.ndarray'>\"}), (<class 'list'> containing values of types {\"<class 'int'>\"})",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[87], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m], optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m70\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint16\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Ai_gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Ai_gpu\\lib\\site-packages\\keras\\engine\\data_adapter.py:1083\u001b[0m, in \u001b[0;36mselect_data_adapter\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m   1080\u001b[0m adapter_cls \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mcls\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01min\u001b[39;00m ALL_ADAPTER_CLS \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mcan_handle(x, y)]\n\u001b[0;32m   1081\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m adapter_cls:\n\u001b[0;32m   1082\u001b[0m     \u001b[38;5;66;03m# TODO(scottzhu): This should be a less implementation-specific error.\u001b[39;00m\n\u001b[1;32m-> 1083\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1084\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to find data adapter that can handle \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1085\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(_type_name(x), _type_name(y))\n\u001b[0;32m   1086\u001b[0m     )\n\u001b[0;32m   1087\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(adapter_cls) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1088\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1089\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData adapters should be mutually exclusive for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1090\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhandling inputs. Found multiple adapters \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m to handle \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1091\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(adapter_cls, _type_name(x), _type_name(y))\n\u001b[0;32m   1092\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to find data adapter that can handle input: (<class 'list'> containing values of types {\"<class 'numpy.ndarray'>\"}), (<class 'list'> containing values of types {\"<class 'int'>\"})"
     ]
    }
   ],
   "source": [
    "model.compile(loss= 'sparse_categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "model.fit(np.concatenate(data, axis=0), np.zeros(70, dtype= np.int16).tolist(), epochs=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c1d4e1ed-ffda-4070-be3b-4ac887efe60d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2817,)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate(data, axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1dcf7278-b901-4899-9cb5-19187e179389",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "30f1be3f-1dcf-4cd6-a5dc-027cc4cdfec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import os\n",
    "import soundfile as sf\n",
    "\n",
    "# Load the audio file\n",
    "audio_file = 'test.mp3'\n",
    "y, sr = librosa.load(audio_file, sr=None, duration=226)\n",
    "# len(y)\n",
    "# # Calculate the length of each segment\n",
    "segment_length = len(y) // 65\n",
    "# segment_length\n",
    "# # Split the audio file into 10 segments and save each segment as a separate file\n",
    "for i in range(65):\n",
    "    segment_start = i * segment_length\n",
    "    segment_end = (i + 1) * segment_length\n",
    "    \n",
    "    # Extract the segment from the audio file\n",
    "    segment = y[segment_start:segment_end]\n",
    "    \n",
    "    # Save the segment as a new audio file\n",
    "    sf.write(f\"Ahmed_Abdelhady.{i}.mp3\", segment, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84eee85c-4195-4cb8-b8af-d880dbf57ff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "count = 0\n",
    "for i in glob.glob('data/*'):\n",
    "    if i.split('.')[0] == 'data\\\\AhmedAbdelhady':\n",
    "        # print(i.split('.')[1])\n",
    "        count+=1\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "788cad4e-0136-4d6b-9059-91e057db40e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data\\\\AhmedAbdelhady', '0', 'mp3']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob.glob('data/*')[100].split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c3130301-94e3-44ec-b1ba-9a547df5e9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "n = 35\n",
    "for i in glob.glob('data/*'):\n",
    "    if i.split('_')[0] == 'data\\\\audio':\n",
    "        os.rename(i,f'data/AhmedAbdelhady.{n}.mp3')\n",
    "        # print(i)\n",
    "        # print(n)\n",
    "        n+=1\n",
    "# os.rename('33.mp3', 'test.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0c1611da-1976-4b76-a6d4-ad968f27da03",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 65\n",
    "for i in glob.glob('*.mp3'):\n",
    "    if int(i.split('.')[0]) in list(range(1, 36)):\n",
    "        os.rename(i, f'AhmedAbdelhady.{n}.mp3')\n",
    "        n+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cd6c6ebc-9def-4c7c-9940-b94fec62384b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(glob.glob('*')[0].split('.')[0]) in list(range(1, 36))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f0a2c690-e7fa-4e40-aaa4-bbb63a567e57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count= 0\n",
    "for i in glob.glob('data/*'):\n",
    "    if i.split('.')[0].split('\\\\')[1] == 'Ahmed_Gamal':\n",
    "        count+=1\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "c265af66-054a-4e68-8b9f-0ea4e2b256ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths = []\n",
    "for i in glob.glob(r\"F:\\ai\\Audio Mnist\\archive\\data/*\"):\n",
    "    path = i+'\\\\5_{}_.5.wav'.format(i.split(\"\\\\\")[-1])\n",
    "    # if int(i.split('_')[1]) in list(range(60)):\n",
    "        # print(i)\n",
    "    # paths.append(i.split(\"_\")[-1].split(\".\")[0])\n",
    "    if os.path.exists(path):\n",
    "        \n",
    "        paths.append(path)\n",
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "cdcabbbc-cd41-4924-945b-46ca5498e8d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'F:\\\\ai\\\\Audio Mnist\\\\archive\\\\data\\\\01'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob.glob(r\"F:\\ai\\Audio Mnist\\archive\\data/*\")[0].split('_')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f02ef19-6748-4bf4-aa6f-9779dcb0d6bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ai_gpu",
   "language": "python",
   "name": "ai_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
